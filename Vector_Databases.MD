Types of Vector Embeddings
Word Embeddings:
• Techniques: Word2 Vec, GloVe, FastText
• Purpose: Capture semantic relationships and contextual information.
Sentence Embeddings:
• Models: Universal Sentence Encoder (USE), Skip Thought
• Purpose: Represent overall meaning and context of sentences.
Document Embeddings:
• Techniques: Doc2Vec, Paragraph Vectors
• Purpose: Capture semantic information and context of entire documents.
Image Embeddings:
• Techniques: CNNs, ResNet, VGG
• Purpose: Capture visual features for tasks like classification and object detection.


#### What is Vector databases?
• A vector database indexes and stores vector embeddings, for fast search and optimized storage
• Provides the ability to compare multiple things (semantically) at the same time
• Helps machine learning models remember past data better, making them more useful for search, recommendations, and text generation
