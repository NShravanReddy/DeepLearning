{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhk0Mu/kgbLivMWCTJz7md",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NShravanReddy/DeepLearning/blob/main/triton/GELU_triton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UtoRGVfQQJI",
        "outputId": "d5846f2c-686f-4fdf-b66f-338ccc7e510f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from large pool |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from large pool |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from large pool |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  12288 MiB |  13314 MiB |  13314 MiB |   1026 MiB |\n",
            "|       from large pool |  12288 MiB |  13312 MiB |  13312 MiB |   1024 MiB |\n",
            "|       from small pool |      0 MiB |      2 MiB |      2 MiB |      2 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |      0 B   |   2047 KiB |   4117 KiB |   4117 KiB |\n",
            "|       from large pool |      0 B   |      0 KiB |      0 KiB |      0 KiB |\n",
            "|       from small pool |      0 B   |   2047 KiB |   4117 KiB |   4117 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       3    |      10    |      48    |      45    |\n",
            "|       from large pool |       3    |       4    |       4    |       1    |\n",
            "|       from small pool |       0    |       7    |      44    |      44    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       3    |      10    |      48    |      45    |\n",
            "|       from large pool |       3    |       4    |       4    |       1    |\n",
            "|       from small pool |       0    |       7    |      44    |      44    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       3    |       5    |       5    |       2    |\n",
            "|       from large pool |       3    |       4    |       4    |       1    |\n",
            "|       from small pool |       0    |       1    |       1    |       1    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       3    |      18    |      18    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       3    |      18    |      18    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "Triton GELU output: tensor([-0.0317, -0.1117, -0.1042,  ..., -0.1676,  0.5474,  1.5746],\n",
            "       device='cuda:0')\n",
            "PyTorch GELU output: tensor([-0.0317, -0.1117, -0.1042,  ..., -0.1676,  0.5474,  1.5746],\n",
            "       device='cuda:0')\n",
            "Are they close? True\n",
            "Triton GELU: [0.1, 0.1, 0.3] (mean 0.194 ms)\n",
            "PyTorch GELU: [0.1, 0.1, 0.1] (mean 0.07 ms)\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  16384 KiB |  13312 MiB |  13389 MiB |  13373 MiB |\n",
            "|       from large pool |  16384 KiB |  13312 MiB |  13384 MiB |  13368 MiB |\n",
            "|       from small pool |      0 KiB |      4 MiB |      5 MiB |      5 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  16384 KiB |  13312 MiB |  13389 MiB |  13373 MiB |\n",
            "|       from large pool |  16384 KiB |  13312 MiB |  13384 MiB |  13368 MiB |\n",
            "|       from small pool |      0 KiB |      4 MiB |      5 MiB |      5 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  16384 KiB |  13312 MiB |  13389 MiB |  13373 MiB |\n",
            "|       from large pool |  16384 KiB |  13312 MiB |  13384 MiB |  13368 MiB |\n",
            "|       from small pool |      0 KiB |      4 MiB |      5 MiB |      5 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  12332 MiB |  13314 MiB |  13358 MiB |   1026 MiB |\n",
            "|       from large pool |  12328 MiB |  13312 MiB |  13352 MiB |   1024 MiB |\n",
            "|       from small pool |      4 MiB |      4 MiB |      6 MiB |      2 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   4096 KiB |  25600 KiB |  99371 KiB |  95275 KiB |\n",
            "|       from large pool |   4096 KiB |  24576 KiB |  86016 KiB |  81920 KiB |\n",
            "|       from small pool |      0 KiB |   2048 KiB |  13355 KiB |  13355 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       4    |      13    |     118    |     114    |\n",
            "|       from large pool |       4    |       9    |      22    |      18    |\n",
            "|       from small pool |       0    |       7    |      96    |      96    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       4    |      13    |     118    |     114    |\n",
            "|       from large pool |       4    |       9    |      22    |      18    |\n",
            "|       from small pool |       0    |       7    |      96    |      96    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       7    |       7    |       9    |       2    |\n",
            "|       from large pool |       5    |       5    |       6    |       1    |\n",
            "|       from small pool |       2    |       2    |       3    |       1    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       1    |       4    |      54    |      53    |\n",
            "|       from large pool |       1    |       2    |      13    |      12    |\n",
            "|       from small pool |       0    |       3    |      41    |      41    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "\n",
            "Average execution time (Forward Pass):\n",
            "  Triton GELU = 0.194 ms\n",
            "  PyTorch GELU = 0.070 ms\n"
          ]
        }
      ],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "\n",
        "@triton.jit\n",
        "def t_g_k(x_ptr, y_ptr, N0, BLOCK_SIZE: tl.constexpr):\n",
        "    pid = tl.program_id(axis=0)\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < N0\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "\n",
        "    # GELU approximation\n",
        "    a = 0.7978845608028654 * (x + 0.044715 * x * x * x)\n",
        "    exp2a = tl.exp(2 * a)\n",
        "    tanha = (exp2a - 1) / (exp2a + 1)\n",
        "    y = 0.5 * x * (1 + tanha)\n",
        "\n",
        "    tl.store(y_ptr + offsets, y, mask=mask)\n",
        "\n",
        "def t_g_k_h(x: torch.Tensor, BLOCK_SIZE=1024):\n",
        "    assert x.is_cuda\n",
        "    y = torch.empty_like(x)\n",
        "    N0 = x.numel()\n",
        "    grid = lambda meta: (triton.cdiv(N0, meta['BLOCK_SIZE']),)\n",
        "    t_g_k[grid](x, y, N0, BLOCK_SIZE=BLOCK_SIZE)\n",
        "    return y\n",
        "\n",
        "def benchmark(description: str, run, num_warmups: int = 1, num_trials: int = 3):\n",
        "    # Warm-up runs\n",
        "    for _ in range(num_warmups):\n",
        "        run()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Timing\n",
        "    times = []\n",
        "    for _ in range(num_trials):\n",
        "        start_time = time.perf_counter()\n",
        "        run()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.perf_counter()\n",
        "\n",
        "        elapsed_ms = (end_time - start_time) * 1000\n",
        "        times.append(elapsed_ms)\n",
        "\n",
        "    mean_time = sum(times) / len(times)\n",
        "\n",
        "    print(f\"{description}: {list(map(lambda x: round(x, 1), sorted(times)))} (mean {round(mean_time, 3)} ms)\")\n",
        "\n",
        "    return mean_time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    N = 1024 * 1024\n",
        "    BLOCK_SIZE = 1024\n",
        "    print(torch.cuda.memory_summary())\n",
        "    torch.cuda.empty_cache()\n",
        "    # Input tensor\n",
        "    x = torch.randn(N, device='cuda', dtype=torch.float32)\n",
        "\n",
        "    # Triton GELU\n",
        "    GELU_triton = t_g_k_h(x, BLOCK_SIZE)\n",
        "    # PyTorch GELU\n",
        "    gelu = nn.GELU(approximate='tanh')\n",
        "\n",
        "    GELU_pytorch = gelu(x)\n",
        "\n",
        "    # Verify correctness\n",
        "    print(\"Triton GELU output:\", GELU_triton)\n",
        "    print(\"PyTorch GELU output:\", GELU_pytorch)\n",
        "    print(\"Are they close?\", torch.allclose(GELU_triton, GELU_pytorch, atol=1e-5))\n",
        "\n",
        "    # Benchmarking forward pass\n",
        "    x_torch = x.detach().clone().requires_grad_()\n",
        "\n",
        "    GELU_pytorch = nn.GELU()\n",
        "\n",
        "    triton_time = benchmark(\"Triton GELU\", lambda: t_g_k_h(x, BLOCK_SIZE))\n",
        "    torch_time = benchmark(\"PyTorch GELU\", lambda: GELU_pytorch(x))\n",
        "    print(torch.cuda.memory_summary())\n",
        "\n",
        "    print(f\"\\nAverage execution time (Forward Pass):\")\n",
        "    print(f\"  Triton GELU = {triton_time:.3f} ms\")\n",
        "    print(f\"  PyTorch GELU = {torch_time:.3f} ms\")\n"
      ]
    }
  ]
}